# 2B

Feature selection is the process of removing irrelevant or redundant features from the original data set. So the execution time of the classifier that processes the data reduces, also accuracy increases because irrelevant features can include noisy data affecting the classification accuracy negatively. With feature selection the understandability can be improved and cost of data handling becomes smaller. 

Feature selection algorithms are divided into three categories; filters, wrappers and embedded selectors. Filters evaluate each feature  independent from the classifier, rank the features after evaluation and take the superior ones. Wrappers takes a subset of the feature set, evaluates the classifier’s performance on this subset, and then another subset is evaluated on the classifier. The subset for which the classifier has the maximum performance is selected. So wrappers are dependent on the selected classifier. In fact wrappers are more reliable because classification algorithm affects the accuracy, although the selection of the subset is an NP-hard problem. So it takes considerable processing time and memory. Some heuristic algorithms can be used for subset selection such as genetic algorithm, greedy stepwise, best first or random search. Therefore, the filters are more time efficient when compared to wrappers, but they don’t take into account that selecting the better features may relevant to classification algorithms. Embedded techniques on the other hand perform feature
selection during learning process like artificial neural networks do. 
